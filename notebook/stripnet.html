<html>
<head>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis/4.16.1/vis.css" type="text/css" />
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/vis/4.16.1/vis-network.min.js"> </script>
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->

<style type="text/css">

        #mynetwork {
            width: 100%;
            height: 750px;
            background-color: #222222;
            border: 1px solid lightgray;
            position: relative;
            float: left;
        }

        

        

        
</style>

</head>

<body>
<div id = "mynetwork"></div>


<script type="text/javascript">

    // initialize global variables.
    var edges;
    var nodes;
    var network; 
    var container;
    var options, data;

    
    // This method is responsible for drawing the graph, returns the drawn network
    function drawGraph() {
        var container = document.getElementById('mynetwork');
        
        

        // parsing and collecting nodes and edges from the python
        nodes = new vis.DataSet([{"font": {"color": "white", "size": 20}, "group": 2, "id": 0, "label": 0, "shape": "dot", "size": 20, "title": "Rethinking Search:  Making Experts out of\u003cbr\u003eDilettantes; Rethinking Search:  Making Experts\u003cbr\u003eout of Dilettantes\u003cbr\u003e\u003cbr\u003eWhen experiencing an\u003cbr\u003einformation need, users want to engage with an\u003cbr\u003eexpert, but often turn to an information retrieval\u003cbr\u003esystem, such as a search engine, instead.\u003cbr\u003eClassical information retrieval systems do not\u003cbr\u003eanswer information needs directly, but instead\u003cbr\u003eprovide references to (hopefully authoritative)\u003cbr\u003eanswers. Successful question answering systems\u003cbr\u003eoffe..."}, {"font": {"color": "white", "size": 20}, "group": 2, "id": 68, "label": 68, "shape": "dot", "size": 20, "title": "Snorkel: Rapid Training Data Creation with Weak\u003cbr\u003eSupervision\u003cbr\u003e\u003cbr\u003eLabeling training data is\u003cbr\u003eincreasingly the largest bottleneck in deploying\u003cbr\u003emachine learning systems. We present Snorkel, a\u003cbr\u003efirst-of-its-kind system that enables users to\u003cbr\u003etrain state-of-the-art models without hand\u003cbr\u003elabeling any training data. Instead, users write\u003cbr\u003elabeling functions that express arbitrary\u003cbr\u003eheuristics, which can have unknown accuracies and\u003cbr\u003ecorrelations. Snorkel denoises their outputs\u003cbr\u003ewith..."}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 1, "label": 1, "shape": "dot", "size": 20, "title": "Scaling Up Visual and Vision-Language\u003cbr\u003eRepresentation Learning With Noisy Text\u003cbr\u003eSupervision\u003cbr\u003e\u003cbr\u003ePre-trained representations are\u003cbr\u003ebecoming crucial for many NLP and perception\u003cbr\u003etasks. While representation learning in NLP has\u003cbr\u003etransitioned to training on raw text without human\u003cbr\u003eannotations, visual and vision-language\u003cbr\u003erepresentations still rely heavily on curated\u003cbr\u003etraining datasets that are expensive or require\u003cbr\u003eexpert knowledge. For vision applications ,\u003cbr\u003erepresentations are..."}, {"font": {"color": "white", "size": 20}, "group": -1, "id": 18, "label": 18, "shape": "dot", "size": 20, "title": "Unsupervised Data Augmentation for Consistency\u003cbr\u003eTraining\u003cbr\u003e\u003cbr\u003eSemi-supervised learning lately\u003cbr\u003ehas shown much promise in improving deep learning\u003cbr\u003emodels when labeled data is scarce. Common among\u003cbr\u003erecent approaches is the use of consistency\u003cbr\u003etraining on a large amount of unlabeled data to\u003cbr\u003econstrain model predictions to be invariant to\u003cbr\u003einput noise. In this work, we present a new\u003cbr\u003eperspective on how to effectively noise unlabeled\u003cbr\u003eexamples and argue that the quality of noising..."}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 34, "label": 34, "shape": "dot", "size": 20, "title": "Learning Transferable Visual Models From Natural\u003cbr\u003eLanguage Supervision\u003cbr\u003e\u003cbr\u003eState-of-the-art\u003cbr\u003ecomputer vision systems are trained to predict a\u003cbr\u003efixed set of predetermined object categories. This\u003cbr\u003erestricted form of supervision limits their\u003cbr\u003egenerality and usability since additional labeled\u003cbr\u003edata is needed to specify any other visual\u003cbr\u003econcept. Learning directly from raw text about\u003cbr\u003eimages is a promising alternative which leverages\u003cbr\u003ea much broader source of supervision. We\u003cbr\u003ed..."}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 40, "label": 40, "shape": "dot", "size": 20, "title": "A Simple Framework for Contrastive Learning of\u003cbr\u003eVisual Representations\u003cbr\u003e\u003cbr\u003eThis paper presents\u003cbr\u003eSimCLR: a simple framework for contrastive\u003cbr\u003elearning of visual representations. We simplify\u003cbr\u003erecently proposed contrastive self-supervised\u003cbr\u003elearning algorithms without requiring specialized\u003cbr\u003earchitectures or a memory bank. In order to\u003cbr\u003eunderstand what enables the contrastive prediction\u003cbr\u003etasks to learn useful representations, we\u003cbr\u003esystematically study the major components of our\u003cbr\u003e..."}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 57, "label": 57, "shape": "dot", "size": 20, "title": "An Image is Worth 16x16 Words: Transformers for\u003cbr\u003eImage Recognition at Scale\u003cbr\u003e\u003cbr\u003eWhile the\u003cbr\u003eTransformer architecture has become the de-facto\u003cbr\u003estandard for natural language processing tasks,\u003cbr\u003eits applications to computer vision remain\u003cbr\u003elimited. In vision, attention is either applied in\u003cbr\u003econjunction with convolutional networks, or used\u003cbr\u003eto replace certain components of convolutional\u003cbr\u003enetworks while keeping their overall structure in\u003cbr\u003eplace. We show that this reliance on CNNs is..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 59, "label": 59, "shape": "dot", "size": 20, "title": "Want To Reduce Labeling Cost? GPT-3 Can\u003cbr\u003eHelp\u003cbr\u003e\u003cbr\u003eData annotation is a time-consuming\u003cbr\u003eand labor-intensive process for many NLP tasks.\u003cbr\u003eAlthough there exist various methods to produce\u003cbr\u003epseudo data labels, they are often task-specific\u003cbr\u003eand require a decent amount of labeled data to\u003cbr\u003estart with. Recently, the immense language model\u003cbr\u003eGPT-3 with 175 billion parameters has achieved\u003cbr\u003etremendous improvement across many few-shot\u003cbr\u003elearning tasks. In this paper, we explore ways to\u003cbr\u003e..."}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 70, "label": 70, "shape": "dot", "size": 20, "title": "Swin Transformer: Hierarchical Vision Transformer\u003cbr\u003eusing Shifted Windows\u003cbr\u003e\u003cbr\u003eThis paper presents a\u003cbr\u003enew vision Transformer, called Swin Transformer,\u003cbr\u003ethat capably serves as a general-purpose backbone\u003cbr\u003efor computer vision. Challenges in adapting\u003cbr\u003eTransformer from language to vision arise from\u003cbr\u003edifferences between the two domains, such as large\u003cbr\u003evariations in the scale of visual entities and the\u003cbr\u003ehigh resolution of pixels in images compared to\u003cbr\u003ewords in text. To address these ..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 4, "label": 4, "shape": "dot", "size": 20, "title": "Knowledge Enhanced Contextual Word\u003cbr\u003eRepresentations\u003cbr\u003e\u003cbr\u003eContextual word\u003cbr\u003erepresentations, typically trained on\u003cbr\u003eunstructured, unlabeled text, do not contain any\u003cbr\u003eexplicit grounding to real world entities and are\u003cbr\u003eoften unable to remember facts about those\u003cbr\u003eentities. We propose a general method to embed\u003cbr\u003emultiple knowledge bases (KBs) into large scale\u003cbr\u003emodels, and thereby enhance their representations\u003cbr\u003ewith structured, human-curated knowledge. For each\u003cbr\u003eKB, we first use an ..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 5, "label": 5, "shape": "dot", "size": 20, "title": "Deep contextualized word representations\u003cbr\u003e\u003cbr\u003eWe\u003cbr\u003eintroduce a new type of deep contextual-ized word\u003cbr\u003erepresentation that models both (1) complex\u003cbr\u003echaracteristics of word use (e.g., syntax and\u003cbr\u003esemantics), and (2) how these uses vary across\u003cbr\u003elinguistic contexts (i.e., to model polysemy). Our\u003cbr\u003eword vectors are learned functions of the internal\u003cbr\u003estates of a deep bidirec-tional language model\u003cbr\u003e(biLM), which is pre-trained on a large text\u003cbr\u003ecorpus. We show that these representation..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 17, "label": 17, "shape": "dot", "size": 20, "title": "Named Entity Recognition for Social Media Texts\u003cbr\u003ewith Semantic Augmentation\u003cbr\u003e\u003cbr\u003eExisting\u003cbr\u003eapproaches for named entity recognition suffer\u003cbr\u003efrom data sparsity problems when conducted on\u003cbr\u003eshort and informal texts, especially user-\u003cbr\u003egenerated social media content. Semantic\u003cbr\u003eaugmentation is a potential way to alleviate this\u003cbr\u003eproblem. Given that rich semantic information is\u003cbr\u003eimplicitly preserved in pre-trained word\u003cbr\u003eembeddings, they are potential ideal resources for\u003cbr\u003esemantic au..."}, {"font": {"color": "white", "size": 20}, "group": -1, "id": 7, "label": 7, "shape": "dot", "size": 20, "title": "Neural Machine Translation of Rare Words with\u003cbr\u003eSubword Units\u003cbr\u003e\u003cbr\u003eNeural machine translation\u003cbr\u003e(NMT) models typically operate with a fixed\u003cbr\u003evocabulary , but translation is an open-vocabulary\u003cbr\u003eproblem. Previous work addresses the translation\u003cbr\u003eof out-of-vocabulary words by backing off to a\u003cbr\u003edictionary. In this paper , we introduce a simpler\u003cbr\u003eand more effective approach, making the NMT model\u003cbr\u003ecapable of open-vocabulary translation by encoding\u003cbr\u003erare and unknown words as sequences ..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 24, "label": 24, "shape": "dot", "size": 20, "title": "skweak: Weak Supervision Made Easy for\u003cbr\u003eNLP\u003cbr\u003e\u003cbr\u003eWe present skweak, a versatile, Python-\u003cbr\u003ebased software toolkit enabling NLP developers to\u003cbr\u003eapply weak supervision to a wide range of NLP\u003cbr\u003etasks. Weak supervision is an emerging machine\u003cbr\u003elearning paradigm based on a simple idea: instead\u003cbr\u003eof labelling data points by hand, we use labelling\u003cbr\u003efunctions derived from domain knowledge to\u003cbr\u003eautomatically obtain annotations for a given\u003cbr\u003edataset. The resulting labels are then aggregated\u003cb..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 45, "label": 45, "shape": "dot", "size": 20, "title": "Isotropy in the Contextual Embedding Space:\u003cbr\u003eClusters and Manifolds\u003cbr\u003e\u003cbr\u003eThe geometric\u003cbr\u003eproperties of contextual embedding spaces for deep\u003cbr\u003elanguage models such as BERT and ERNIE, have\u003cbr\u003eattracted considerable attention in recent years.\u003cbr\u003eInvestigations on the contextual......"}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 47, "label": 47, "shape": "dot", "size": 20, "title": "SimCSE: Simple Contrastive Learning of Sentence\u003cbr\u003eEmbeddings\u003cbr\u003e\u003cbr\u003eThis paper presents SimCSE, a\u003cbr\u003esimple contrastive learning framework that greatly\u003cbr\u003eadvances the state-of-the-art sentence embeddings.\u003cbr\u003eWe first describe an unsupervised approach, which\u003cbr\u003etakes an input sentence and predicts itself in a\u003cbr\u003econtrastive objective, with only standard dropout\u003cbr\u003eused as noise. This simple method works\u003cbr\u003esurprisingly well, performing on par with previous\u003cbr\u003esupervised counterparts. We find th..."}, {"font": {"color": "white", "size": 20}, "group": -1, "id": 8, "label": 8, "shape": "dot", "size": 20, "title": "FNet: Mixing Tokens with Fourier\u003cbr\u003eTransforms\u003cbr\u003e\u003cbr\u003eWe show that Transformer encoder\u003cbr\u003earchitec-tures can be massively sped up, with\u003cbr\u003elimited accuracy costs, by replacing the self-\u003cbr\u003eattention sublayers with simple linear\u003cbr\u003etransformations that \"mix\" input tokens. These\u003cbr\u003elinear transformations , along with simple\u003cbr\u003enonlinearities in feed-forward layers, are\u003cbr\u003esufficient to model semantic relationships in\u003cbr\u003eseveral text classification tasks. Perhaps most\u003cbr\u003esurprisingly, we find that ..."}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 15, "label": 15, "shape": "dot", "size": 20, "title": "Self-Attention Between Datapoints: Going Beyond\u003cbr\u003eIndividual Input-Output Pairs in Deep\u003cbr\u003eLearning\u003cbr\u003e\u003cbr\u003eWe challenge a common assumption\u003cbr\u003eunderlying most supervised deep learning: that a\u003cbr\u003emodel makes a prediction depending only on its\u003cbr\u003eparameters and the features of a single input. To\u003cbr\u003ethis end, we introduce a general-purpose deep\u003cbr\u003elearning architecture that takes as input the\u003cbr\u003eentire dataset instead of processing one datapoint\u003cbr\u003eat a time. Our approach uses self-attention to\u003cbr..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 10, "label": 10, "shape": "dot", "size": 20, "title": "RoBERTa: A Robustly Optimized BERT Pretraining\u003cbr\u003eApproach\u003cbr\u003e\u003cbr\u003eLanguage model pretraining has led\u003cbr\u003eto significant performance gains but careful\u003cbr\u003ecomparison between different approaches is\u003cbr\u003echallenging. Training is computationally expensive\u003cbr\u003e, often done on private datasets of different\u003cbr\u003esizes, and, as we will show, hyperparameter\u003cbr\u003echoices have significant impact on the final\u003cbr\u003eresults. We present a replication study of BERT\u003cbr\u003epretraining (Devlin et al., 2019) that carefully\u003cbr\u003em..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 60, "label": 60, "shape": "dot", "size": 20, "title": "AMMUS : A Survey of Transformer-based Pretrained\u003cbr\u003eModels in Natural Language\u003cbr\u003eProcessing\u003cbr\u003e\u003cbr\u003eTransformer-based pretrained\u003cbr\u003elanguage models (T-PTLMs) have achieved great\u003cbr\u003esuccess in almost every NLP task. The evolution of\u003cbr\u003ethese models started with GPT and BERT. These\u003cbr\u003emodels are built on the top of transformers, self-\u003cbr\u003esupervised learning and transfer learning.\u003cbr\u003eTransformed-based PTLMs learn universal language\u003cbr\u003erepresentations from large volumes of text data\u003cbr\u003eusing self-su..."}, {"font": {"color": "white", "size": 20}, "group": -1, "id": 14, "label": 14, "shape": "dot", "size": 20, "title": "DocBERT: BERT for Document\u003cbr\u003eClassification\u003cbr\u003e\u003cbr\u003eWe present, to our\u003cbr\u003eknowledge, the first application of BERT to\u003cbr\u003edocument classification. A few characteristics of\u003cbr\u003ethe task might lead one to think that BERT is not\u003cbr\u003ethe most appropriate model: syntactic structures\u003cbr\u003ematter less for content categories, documents can\u003cbr\u003eoften be longer than typical BERT input, and\u003cbr\u003edocuments often have multiple labels.\u003cbr\u003eNevertheless, we show that a straightforward\u003cbr\u003eclassification model using BERT..."}, {"font": {"color": "white", "size": 20}, "group": -1, "id": 21, "label": 21, "shape": "dot", "size": 20, "title": "Small-text: Active Learning for Text\u003cbr\u003eClassification in Python\u003cbr\u003e\u003cbr\u003eWe present small-\u003cbr\u003etext, a simple modular active learning library,\u003cbr\u003ewhich offers pool-based active learning for text\u003cbr\u003eclassification in Python. It comes with various\u003cbr\u003epre-implemented state-of-the-art query strategies,\u003cbr\u003eincluding some which can leverage the GPU. Clearly\u003cbr\u003edefined interfaces allow to combine a multitude of\u003cbr\u003esuch query strategies with different classifiers,\u003cbr\u003ethereby facilitating a quick mix and m..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 44, "label": 44, "shape": "dot", "size": 20, "title": "A Multi-task Approach for Named Entity Recognition\u003cbr\u003ein Social Media Data\u003cbr\u003e\u003cbr\u003eNamed Entity\u003cbr\u003eRecognition for social media data is challenging\u003cbr\u003ebecause of its inherent noisiness. In addition to\u003cbr\u003eimproper grammatical structures, it contains\u003cbr\u003espelling inconsistencies and numerous informal\u003cbr\u003eabbreviations. We propose a novel multi-task\u003cbr\u003eapproach by employing a more general secondary\u003cbr\u003etask of Named Entity (NE) segmentation together\u003cbr\u003ewith the primary task of fine-grained NE\u003cbr\u003ecatego..."}, {"font": {"color": "white", "size": 20}, "group": 2, "id": 22, "label": 22, "shape": "dot", "size": 20, "title": "Adversarial Multiclass Learning under Weak\u003cbr\u003eSupervision with Performance Guarantees\u003cbr\u003e\u003cbr\u003eWe\u003cbr\u003edevelop a rigorous approach for using a set of\u003cbr\u003earbitrarily correlated weak supervision sources in\u003cbr\u003eorder to solve a multiclass classi\ufb01cation task\u003cbr\u003ewhen only a very small set of labeled data is\u003cbr\u003eavailable. Our learning algorithm provably\u003cbr\u003econverges to a model that has minimum empirical\u003cbr\u003erisk with respect to an adversarial choice over\u003cbr\u003efeasible labelings for a set of unlabeled data,\u003cbr..."}, {"font": {"color": "white", "size": 20}, "group": 2, "id": 25, "label": 25, "shape": "dot", "size": 20, "title": "Training Complex Models with Multi-Task Weak\u003cbr\u003eSupervision\u003cbr\u003e\u003cbr\u003eAs machine learning models\u003cbr\u003econtinue to increase in complexity, collecting\u003cbr\u003elarge hand-labeled training sets has become one of\u003cbr\u003ethe biggest roadblocks in practice. Instead,\u003cbr\u003eweaker forms of supervision that provide noisier\u003cbr\u003ebut cheaper labels are often used. However, these\u003cbr\u003eweak supervision sources have diverse and unknown\u003cbr\u003eaccuracies, may output correlated labels, and may\u003cbr\u003elabel different tasks or apply at differ..."}, {"font": {"color": "white", "size": 20}, "group": 2, "id": 26, "label": 26, "shape": "dot", "size": 20, "title": "Data Programming: Creating Large Training Sets,\u003cbr\u003eQuickly\u003cbr\u003e\u003cbr\u003eLarge labeled training sets are the\u003cbr\u003ecritical building blocks of supervised learning\u003cbr\u003emethods and are key enablers of deep learning\u003cbr\u003etechniques. For some applications, creating\u003cbr\u003elabeled training sets is the most time-consuming\u003cbr\u003eand expensive part of applying machine learning.\u003cbr\u003eWe therefore propose a paradigm for the\u003cbr\u003eprogrammatic creation of training sets called data\u003cbr\u003eprogramming in which users express weak\u003cbr\u003esupe..."}, {"font": {"color": "white", "size": 20}, "group": 2, "id": 53, "label": 53, "shape": "dot", "size": 20, "title": "End-to-End Weak Supervision\u003cbr\u003e\u003cbr\u003eAggregating\u003cbr\u003emultiple sources of weak supervision (WS) can ease\u003cbr\u003ethe data-labeling bottleneck prevalent in many\u003cbr\u003emachine learning applications, by replacing the\u003cbr\u003etedious manual collection of ground truth labels.\u003cbr\u003eCurrent state of the art approaches that do not\u003cbr\u003euse any labeled training data, however, require\u003cbr\u003etwo separate modeling steps: Learning a\u003cbr\u003eprobabilistic latent variable model based on the\u003cbr\u003eWS sources -- making assumptions that rarely ..."}, {"font": {"color": "white", "size": 20}, "group": 2, "id": 66, "label": 66, "shape": "dot", "size": 20, "title": "Snuba: Automating Weak Supervision to Label\u003cbr\u003eTraining Data\u003cbr\u003e\u003cbr\u003eAs deep learning models are\u003cbr\u003eapplied to increasingly diverse problems, a key\u003cbr\u003ebottleneck is gathering enough high-quality\u003cbr\u003etraining labels tailored to each task. Users\u003cbr\u003etherefore turn to weak supervision, relying on\u003cbr\u003eimperfect sources of labels like pattern matching\u003cbr\u003eand user-de\ufb01ned heuristics. Unfortunately, users\u003cbr\u003ehave to design these sources for each task. This\u003cbr\u003eprocess can be time consuming and expensive:\u003cbr\u003e..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 19, "label": 19, "shape": "dot", "size": 20, "title": "A Visual Survey of Data Augmentation in\u003cbr\u003eNLP\u003cbr\u003e\u003cbr\u003eAn extensive overview of text data\u003cbr\u003eaugmentation techniques for Natural Language\u003cbr\u003eProcessing..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 27, "label": 27, "shape": "dot", "size": 20, "title": "A Survey of Data Augmentation Approaches for\u003cbr\u003eNLP\u003cbr\u003e\u003cbr\u003eData augmentation has recently seen\u003cbr\u003eincreased interest in NLP due to more work in low-\u003cbr\u003eresource domains, new tasks, and the popularity of\u003cbr\u003elarge-scale neural networks that require large\u003cbr\u003eamounts of training data. Despite this recent\u003cbr\u003eupsurge, this area is still relatively\u003cbr\u003eunderexplored, perhaps due to the challenges posed\u003cbr\u003eby the discrete nature of language data. In this\u003cbr\u003epaper, we present a comprehensive and unifying\u003c..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 20, "label": 20, "shape": "dot", "size": 20, "title": "Dice Loss for Data-imbalanced NLP\u003cbr\u003eTasks\u003cbr\u003e\u003cbr\u003eMany NLP tasks such as tagging and\u003cbr\u003emachine reading comprehension are faced with the\u003cbr\u003esevere data imbalance issue: negative examples\u003cbr\u003esignificantly outnumber positive examples, and the\u003cbr\u003ehuge number of background examples (or easy-\u003cbr\u003enegative examples) overwhelms the training. The\u003cbr\u003emost commonly used cross entropy (CE) criteria is\u003cbr\u003eactually an accuracy-oriented objective, and thus\u003cbr\u003ecreates a discrepancy between training and test:\u003cbr..."}, {"font": {"color": "white", "size": 20}, "group": 2, "id": 67, "label": 67, "shape": "dot", "size": 20, "title": "Interactive Programmatic Labeling for Weak\u003cbr\u003eSupervision\u003cbr\u003e\u003cbr\u003eThe standard supervised machine\u003cbr\u003elearning pipeline involves labeling individual\u003cbr\u003etraining data points, which is often prohibitively\u003cbr\u003eslow and expensive. New programmatic or weak\u003cbr\u003esupervision approaches expedite this process by\u003cbr\u003ehaving users instead write labeling functions,\u003cbr\u003esimple rules or other heuristic functions that\u003cbr\u003elabel subsets of a dataset. While these types of\u003cbr\u003eprogrammatic labeling approaches can provide\u003c..."}, {"font": {"color": "white", "size": 20}, "group": 2, "id": 65, "label": 65, "shape": "dot", "size": 20, "title": "Snorkel: Fast Training Set Generation for\u003cbr\u003eInformation Extraction\u003cbr\u003e\u003cbr\u003eState-of-the art\u003cbr\u003emachine learning methods such as deep learning\u003cbr\u003erely on large sets of hand-labeled training data.\u003cbr\u003eCollecting training data is prohibitively slow and\u003cbr\u003eexpensive, especially when technical domain\u003cbr\u003eexpertise is required; even the largest technology\u003cbr\u003ecompanies struggle with this challenge1. We\u003cbr\u003eaddress this critical bottleneck with Snorkel2, a\u003cbr\u003enew system for quickly creating, managing, and\u003c..."}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 28, "label": 28, "shape": "dot", "size": 20, "title": "A Comprehensive Survey and Experimental Comparison\u003cbr\u003eof Graph-Based Approximate Nearest Neighbor\u003cbr\u003eSearch\u003cbr\u003e\u003cbr\u003eApproximate nearest neighbor search\u003cbr\u003e(ANNS) constitutes an important operation in a\u003cbr\u003emultitude of applications, including\u003cbr\u003erecommendation systems, information retrieval, and\u003cbr\u003epattern recognition. In the past decade, graph-\u003cbr\u003ebased ANNS algorithms have been the leading\u003cbr\u003eparadigm in this domain, with dozens of graph-\u003cbr\u003ebased ANNS algorithms proposed. Such algorithms\u003cbr\u003eaim..."}, {"font": {"color": "white", "size": 20}, "group": -1, "id": 29, "label": 29, "shape": "dot", "size": 20, "title": "A Survey on Locality Sensitive Hashing Algorithms\u003cbr\u003eand their Applications\u003cbr\u003e\u003cbr\u003eFinding nearest\u003cbr\u003eneighbors in high-dimensional spaces is a\u003cbr\u003efundamental operation in many diverse application\u003cbr\u003edomains. Locality Sensitive Hashing (LSH) is one\u003cbr\u003eof the most popular techniques for finding\u003cbr\u003eapproximate nearest neighbor searches in high-\u003cbr\u003edimensional spaces. The main benefits of LSH are\u003cbr\u003eits sub-linear query performance and theoretical\u003cbr\u003eguarantees on the query accuracy. In this survey..."}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 30, "label": 30, "shape": "dot", "size": 20, "title": "Leveraging Reinforcement Learning for evaluating\u003cbr\u003eRobustness of KNN Search Algorithms\u003cbr\u003e\u003cbr\u003eThe\u003cbr\u003eproblem of finding K-nearest neighbors in the\u003cbr\u003egiven dataset for a given query point has been\u003cbr\u003eworked upon since several years. In very high\u003cbr\u003edimensional spaces the K-nearest neighbor search\u003cbr\u003e(KNNS) suffers in terms of complexity in\u003cbr\u003ecomputation of high dimensional distances. With\u003cbr\u003ethe issue of curse of dimensionality, it gets\u003cbr\u003equite tedious to reliably bank on the results of\u003cbr\u003eva..."}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 31, "label": 31, "shape": "dot", "size": 20, "title": "Approximate Nearest Neighbor Search in High\u003cbr\u003eDimensions\u003cbr\u003e\u003cbr\u003eThe nearest neighbor problem is\u003cbr\u003edefined as follows: Given a set $P$ of $n$ points\u003cbr\u003ein some metric space $(X,D)$, build a data\u003cbr\u003estructure that, given any point $q$, returns a\u003cbr\u003epoint in $P$ that is closest to $q$ (its \"nearest\u003cbr\u003eneighbor\" in $P$). The data structure stores\u003cbr\u003eadditional information about the set $P$, which is\u003cbr\u003ethen used to find the nearest neighbor without\u003cbr\u003ecomputing all distances between $q$ and $P$. T..."}, {"font": {"color": "white", "size": 20}, "group": -1, "id": 32, "label": 32, "shape": "dot", "size": 20, "title": "Hashing for Similarity Search: A\u003cbr\u003eSurvey\u003cbr\u003e\u003cbr\u003eSimilarity search (nearest neighbor\u003cbr\u003esearch) is a problem of pursuing the data items\u003cbr\u003ewhose distances to a query item are the smallest\u003cbr\u003efrom a large database. Various methods have been\u003cbr\u003edeveloped to address this problem, and recently a\u003cbr\u003elot of efforts have been devoted to approximate\u003cbr\u003esearch. In this paper, we present a survey on one\u003cbr\u003eof the main solutions, hashing, which has been\u003cbr\u003ewidely studied since the pioneering work locality..."}, {"font": {"color": "white", "size": 20}, "group": -1, "id": 33, "label": 33, "shape": "dot", "size": 20, "title": "Experimental Analysis of Locality Sensitive\u003cbr\u003eHashing Techniques for High-Dimensional\u003cbr\u003eApproximate Nearest Neighbor\u003cbr\u003eSearches\u003cbr\u003e\u003cbr\u003eFinding nearest neighbors in high-\u003cbr\u003edimensional spaces is a fundamental operation in\u003cbr\u003emany multimedia retrieval applications. Exact\u003cbr\u003etree-based indexing approaches are known to suffer\u003cbr\u003efrom the notorious curse of dimensionality for\u003cbr\u003ehigh-dimensional data. Approximate searching\u003cbr\u003etechniques sacrifice some accuracy while returning\u003cbr\u003egood enough resul..."}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 56, "label": 56, "shape": "dot", "size": 20, "title": "Fast Approximate Nearest Neighbor Search With The\u003cbr\u003eNavigating Spreading-out Graph\u003cbr\u003e\u003cbr\u003eApproximate\u003cbr\u003enearest neighbor search (ANNS) is a fundamental\u003cbr\u003eproblem in databases and data mining. A scalable\u003cbr\u003eANNS algorithm should be both memory-efficient and\u003cbr\u003efast. Some early graph-based approaches have shown\u003cbr\u003eattractive theoretical guarantees on search time\u003cbr\u003ecomplexity, but they all suffer from the problem\u003cbr\u003eof high indexing time complexity. Recently, some\u003cbr\u003egraph-based methods have be..."}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 61, "label": 61, "shape": "dot", "size": 20, "title": "LANNS: A Web-Scale Approximate Nearest Neighbor\u003cbr\u003eLookup System\u003cbr\u003e\u003cbr\u003eNearest neighbor search (NNS)\u003cbr\u003ehas a wide range of applications in information\u003cbr\u003eretrieval, computer vision, machine learning,\u003cbr\u003edatabases, and other areas. Existing state-of-the-\u003cbr\u003eart algorithm for nearest neighbor search,\u003cbr\u003eHierarchical Navigable Small World Networks(HNSW),\u003cbr\u003eis unable to scale to large datasets of 100M\u003cbr\u003erecords in high dimensions. In this paper, we\u003cbr\u003epropose LANNS, an end-to-end platform for\u003cbr..."}, {"font": {"color": "white", "size": 20}, "group": -1, "id": 42, "label": 42, "shape": "dot", "size": 20, "title": "The 2021 Image Similarity Dataset and\u003cbr\u003eChallenge\u003cbr\u003e\u003cbr\u003eThis paper introduces a new\u003cbr\u003ebenchmark for large-scale image similarity\u003cbr\u003edetection. This benchmark is used for the Image\u003cbr\u003eSimilarity Challenge at NeurIPS\u002721 (ISC2021). The\u003cbr\u003egoal is to determine whether a query image is a\u003cbr\u003emodified copy of any image in a reference corpus\u003cbr\u003eof size 1~million. The benchmark features a\u003cbr\u003evariety of image transformations such as automated\u003cbr\u003etransformations, hand-crafted image edits and\u003cbr\u003emachine-..."}, {"font": {"color": "white", "size": 20}, "group": -1, "id": 35, "label": 35, "shape": "dot", "size": 20, "title": "2nd Place Solution to Facebook AI Image Similarity\u003cbr\u003eChallenge Matching Track\u003cbr\u003e\u003cbr\u003eThis paper\u003cbr\u003epresents the 2nd place solution to the Facebook AI\u003cbr\u003eImage Similarity Challenge : Matching Track on\u003cbr\u003eDrivenData. The solution is based on self-\u003cbr\u003esupervised learning, and Vision Transformer(ViT).\u003cbr\u003eThe main breaktrough comes from concatenating\u003cbr\u003equery and reference image to form as one image and\u003cbr\u003easking ViT to directly predict from the image if\u003cbr\u003equery image used reference image. The solu..."}, {"font": {"color": "white", "size": 20}, "group": -1, "id": 36, "label": 36, "shape": "dot", "size": 20, "title": "D$^2$LV: A Data-Driven and Local-Verification\u003cbr\u003eApproach for Image Copy Detection\u003cbr\u003e\u003cbr\u003eImage\u003cbr\u003ecopy detection is of great importance in real-life\u003cbr\u003esocial media. In this paper, a data-driven and\u003cbr\u003elocal-verification (D$^2$LV) approach is proposed\u003cbr\u003eto compete for Image Similarity Challenge:\u003cbr\u003eMatching Track at NeurIPS\u002721. In D$^2$LV,\u003cbr\u003eunsupervised pre-training substitutes the\u003cbr\u003ecommonly-used supervised one. When training, we\u003cbr\u003edesign a set of basic and six advanced\u003cbr\u003etransformations..."}, {"font": {"color": "white", "size": 20}, "group": -1, "id": 37, "label": 37, "shape": "dot", "size": 20, "title": "D$^2$LV: A Data-Driven and Local-Verification\u003cbr\u003eApproach for Image Copy Detection\u003cbr\u003e\u003cbr\u003eImage\u003cbr\u003ecopy detection is of great importance in real-life\u003cbr\u003esocial media. In this paper, a data-driven and\u003cbr\u003elocal-verification (D$^2$LV) approach is proposed\u003cbr\u003eto compete for Image Similarity Challenge:\u003cbr\u003eMatching Track at NeurIPS\u002721. In D$^2$LV,\u003cbr\u003eunsupervised pre-training substitutes the\u003cbr\u003ecommonly-used supervised one. When training, we\u003cbr\u003edesign a set of basic and six advanced\u003cbr\u003etransformations..."}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 41, "label": 41, "shape": "dot", "size": 20, "title": "Contrastive Learning with Large Memory Bank and\u003cbr\u003eNegative Embedding Subtraction for Accurate Copy\u003cbr\u003eDetection\u003cbr\u003e\u003cbr\u003eCopy detection, which is a task\u003cbr\u003eto determine whether an image is a modified copy\u003cbr\u003eof any image in a database, is an unsolved\u003cbr\u003eproblem. Thus, we addressed copy detection by\u003cbr\u003etraining convolutional neural networks (CNNs) with\u003cbr\u003econtrastive learning. Training with a large\u003cbr\u003ememory-bank and hard data augmentation enables the\u003cbr\u003eCNNs to obtain more discriminative represen..."}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 38, "label": 38, "shape": "dot", "size": 20, "title": "Deep Clustering for Unsupervised Learning of\u003cbr\u003eVisual Features\u003cbr\u003e\u003cbr\u003eClustering is a class of\u003cbr\u003eunsupervised learning methods that has been\u003cbr\u003eextensively applied and studied in computer\u003cbr\u003evision. Little work has been done to adapt it to\u003cbr\u003ethe end-to-end training of visual features on\u003cbr\u003elarge scale datasets. In this work, we present\u003cbr\u003eDeepCluster, a clustering method that jointly\u003cbr\u003elearns the parameters of a neural network and the\u003cbr\u003ecluster assignments of the resulting features.\u003cbr\u003eDeep..."}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 51, "label": 51, "shape": "dot", "size": 20, "title": "Contrastive Representation Learning\u003cbr\u003e\u003cbr\u003eThe\u003cbr\u003emain idea of contrastive learning is to learn\u003cbr\u003erepresentations such that similar samples stay\u003cbr\u003eclose to each other, while dissimilar ones are far\u003cbr\u003eapart. Contrastive learning can be applied to both\u003cbr\u003esupervised and unsupervised data and has been\u003cbr\u003eshown to achieve good performance on a variety of\u003cbr\u003evision and......"}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 39, "label": 39, "shape": "dot", "size": 20, "title": "Momentum Contrast for Unsupervised Visual\u003cbr\u003eRepresentation Learning\u003cbr\u003e\u003cbr\u003eWe present Momentum\u003cbr\u003eContrast (MoCo) for unsupervised visual\u003cbr\u003erepresentation learning. From a perspective on\u003cbr\u003econtrastive learning as dictionary look-up, we\u003cbr\u003ebuild a dynamic dictionary with a queue and a\u003cbr\u003emoving-averaged encoder. This enables building a\u003cbr\u003elarge and consistent dictionary on-the-fly that\u003cbr\u003efacilitates contrastive unsupervised learning.\u003cbr\u003eMoCo provides competitive results under the common\u003cbr\u003eli..."}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 52, "label": 52, "shape": "dot", "size": 20, "title": "Intriguing Properties of Contrastive\u003cbr\u003eLosses\u003cbr\u003e\u003cbr\u003eWe study three intriguing properties\u003cbr\u003eof contrastive learning. First, we generalize the\u003cbr\u003estandard contrastive loss to a broader family of\u003cbr\u003elosses, and we find that various instantiations of\u003cbr\u003ethe generalized loss perform similarly under the\u003cbr\u003epresence of a multi-layer non-linear projection\u003cbr\u003ehead. Second, we study if instance-based\u003cbr\u003econtrastive learning (with a global image\u003cbr\u003erepresentation) can learn well on images with\u003cbr\u003emultip..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 43, "label": 43, "shape": "dot", "size": 20, "title": "Sentence-BERT: Sentence Embeddings using Siamese\u003cbr\u003eBERT-Networks\u003cbr\u003e\u003cbr\u003eBERT (Devlin et al., 2018)\u003cbr\u003eand RoBERTa (Liu et al., 2019) has set a new\u003cbr\u003estate-of-the-art performance on sentence-pair\u003cbr\u003eregression tasks like semantic textual similarity\u003cbr\u003e(STS). However, it requires that both sentences\u003cbr\u003eare fed into the network, which causes a massive\u003cbr\u003ecomputational overhead: Finding the most similar\u003cbr\u003epair in a collection of 10,000 sentences requires\u003cbr\u003eabout 50 million inference computations..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 48, "label": 48, "shape": "dot", "size": 20, "title": "An Unsupervised Sentence Embedding Method by\u003cbr\u003eMutual Information Maximization\u003cbr\u003e\u003cbr\u003eBERT is\u003cbr\u003einefficient for sentence-pair tasks such as\u003cbr\u003eclustering or semantic search as it needs to\u003cbr\u003eevaluate combinatorially many sentence pairs which\u003cbr\u003eis very time-consuming. Sentence BERT (SBERT)\u003cbr\u003eattempted to solve this challenge by learning\u003cbr\u003esemantically meaningful representations of single\u003cbr\u003esentences, such that similarity comparison can be\u003cbr\u003eeasily accessed. However, SBERT is trained on\u003cbr\u003e..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 49, "label": 49, "shape": "dot", "size": 20, "title": "TSDAE: Using Transformer-based Sequential\u003cbr\u003eDenoising Auto-Encoder for Unsupervised Sentence\u003cbr\u003eEmbedding Learning\u003cbr\u003e\u003cbr\u003eLearning sentence\u003cbr\u003eembeddings often requires a large amount of\u003cbr\u003elabeled data. However, for most tasks and domains,\u003cbr\u003elabeled data is seldom available and creating it\u003cbr\u003eis expensive. In this work, we present a new\u003cbr\u003estate-of-the-art unsupervised method based on pre-\u003cbr\u003etrained Transformers and Sequential Denoising\u003cbr\u003eAuto-Encoder (TSDAE) which outperforms previous\u003cbr\u003ea..."}, {"font": {"color": "white", "size": 20}, "group": -1, "id": 54, "label": 54, "shape": "dot", "size": 20, "title": "Multi-Vector Models with Textual Guidance for\u003cbr\u003eFine-Grained Scientific Document\u003cbr\u003eSimilarity\u003cbr\u003e\u003cbr\u003eWe present Aspire, a new\u003cbr\u003escientific document similarity model based on\u003cbr\u003ematching fine-grained aspects. Our model is\u003cbr\u003etrained using co-citation contexts that describe\u003cbr\u003erelated paper aspects as a novel form of textual\u003cbr\u003esupervision. We use multi-vector document\u003cbr\u003erepresentations, recently explored in settings\u003cbr\u003ewith short query texts but under-explored in the\u003cbr\u003echallenging document-d..."}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 62, "label": 62, "shape": "dot", "size": 20, "title": "Announcing ScaNN: Efficient Vector Similarity\u003cbr\u003eSearch\u003cbr\u003e\u003cbr\u003ePosted by Philip Sun, Software\u003cbr\u003eEngineer, Google Research     Suppose one wants to\u003cbr\u003esearch through a large dataset of literary works\u003cbr\u003eusing que......"}, {"font": {"color": "white", "size": 20}, "group": 1, "id": 58, "label": 58, "shape": "dot", "size": 20, "title": "Milvus: A Purpose-Built Vector Data Management\u003cbr\u003eSystem\u003cbr\u003e\u003cbr\u003eRecently, there has been a pressing\u003cbr\u003eneed to manage high-dimensional vector data in\u003cbr\u003edata science and AI applications. This trend is\u003cbr\u003efueled by the proliferation of unstructured data\u003cbr\u003eand machine learning (ML), where ML models usually\u003cbr\u003etransform unstructured data into feature vectors\u003cbr\u003efor data analytics, e.g., product recommendation.\u003cbr\u003eExisting systems and algorithms for managing\u003cbr\u003evector data have two limitations: (1)..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 2, "label": 2, "shape": "dot", "size": 20, "title": "Machine learning as a model for cultural learning:\u003cbr\u003eTeaching an algorithm what it means to be\u003cbr\u003efat\u003cbr\u003e\u003cbr\u003eAs we navigate our cultural\u003cbr\u003eenvironment, we learn cultural biases, like those\u003cbr\u003earound gender, social class, health, and body\u003cbr\u003eweight. It is unclear, however, exactly how public\u003cbr\u003eculture becomes private culture. In this paper, we\u003cbr\u003eprovide a theoretical account of such cultural\u003cbr\u003elearning. We propose that neural word embeddings\u003cbr\u003eprovide a parsimonious and cognitively plausibl..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 3, "label": 3, "shape": "dot", "size": 20, "title": "Context-Guided BERT for Targeted Aspect-Based\u003cbr\u003eSentiment Analysis\u003cbr\u003e\u003cbr\u003eAspect-based sentiment\u003cbr\u003eanalysis (ABSA) and Targeted ABSA (TABSA) allow\u003cbr\u003efiner-grained inferences about sentiment to be\u003cbr\u003edrawn from the same text, depending on context.\u003cbr\u003eFor example, a given text can have different\u003cbr\u003etargets (e.g., neighborhoods) and different\u003cbr\u003easpects (e.g., price or safety), with different\u003cbr\u003esentiment associated with each target-aspect pair.\u003cbr\u003eIn this paper, we investigate whether adding\u003cbr..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 6, "label": 6, "shape": "dot", "size": 20, "title": "Lexical Normalisation of Twitter\u003cbr\u003eData\u003cbr\u003e\u003cbr\u003eTwitter with over 500 million users\u003cbr\u003eglobally, generates over 100,000 tweets per minute\u003cbr\u003e1. The 140 character limit per tweet, perhaps\u003cbr\u003eunintentionally, encourages users to use shorthand\u003cbr\u003enotations and to strip spellings to their bare\u003cbr\u003eminimum \"syllables\" or elisions e.g. \"srsly\". The\u003cbr\u003eanalysis of Twitter messages which typically\u003cbr\u003econtain misspellings, elisions, and grammatical\u003cbr\u003eerrors, poses a challenge to established Natural\u003cbr\u003eLa..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 9, "label": 9, "shape": "dot", "size": 20, "title": "KG-BERT: BERT for Knowledge Graph\u003cbr\u003eCompletion\u003cbr\u003e\u003cbr\u003eKnowledge graphs are important\u003cbr\u003eresources for many artificial intelligence tasks\u003cbr\u003ebut often suffer from incompleteness. In this\u003cbr\u003ework, we propose to use pre-trained language\u003cbr\u003emodels for knowledge graph completion. We treat\u003cbr\u003etriples in knowledge graphs as textual sequences\u003cbr\u003eand propose a novel framework named Knowledge\u003cbr\u003eGraph Bidirectional Encoder Representations from\u003cbr\u003eTransformer (KG-BERT) to model these triples. Our\u003cbr\u003emetho..."}, {"font": {"color": "white", "size": 20}, "group": -1, "id": 11, "label": 11, "shape": "dot", "size": 20, "title": "A Flexible Large-Scale Similar Product\u003cbr\u003eIdentification Sys-tem in\u003cbr\u003eE-commerce\u003cbr\u003e\u003cbr\u003eIdentifying similar products is\u003cbr\u003ea common pain-point in the world of E-commerce\u003cbr\u003esearch and discovery. The key challenges lie in\u003cbr\u003etwo aspects: 1) The definition of similarity\u003cbr\u003evaries across different applications , such as\u003cbr\u003enear identical products sold by different vendors\u003cbr\u003e, products that are substitutable to each other\u003cbr\u003efor customers with common interests, personalized\u003cbr\u003eproducts visually si..."}, {"font": {"color": "white", "size": 20}, "group": -1, "id": 12, "label": 12, "shape": "dot", "size": 20, "title": "Semi-supervised Category-specific Review Tagging\u003cbr\u003eon Indonesian E-Commerce Product\u003cbr\u003eReviews\u003cbr\u003e\u003cbr\u003eProduct reviews are a huge source\u003cbr\u003eof natural language data in e-commerce\u003cbr\u003eapplications. Several millions of customers write\u003cbr\u003ereviews regarding a variety of topics. We\u003cbr\u003ecategorize these topics into two groups as either\u003cbr\u003e\u201ccategory-specific\u201d topics or as \u201cgeneric\u201d topics\u003cbr\u003ethat span multiple product categories. While we\u003cbr\u003ecan use a supervised learning approach to tag\u003cbr\u003ereview text fo..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 13, "label": 13, "shape": "dot", "size": 20, "title": "Graph Neural Networks for Natural Language\u003cbr\u003eProcessing: A Survey\u003cbr\u003e\u003cbr\u003eDeep learning has\u003cbr\u003ebecome the dominant approach in coping with\u003cbr\u003evarious tasks in Natural LanguageProcessing (NLP).\u003cbr\u003eAlthough text inputs are typically represented as\u003cbr\u003ea sequence of tokens, there isa rich variety of\u003cbr\u003eNLP problems that can be best expressed with a\u003cbr\u003egraph structure. As a result, thereis a surge of\u003cbr\u003einterests in developing new deep learning\u003cbr\u003etechniques on graphs for a large numberof NLP\u003cbr\u003etask..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 16, "label": 16, "shape": "dot", "size": 20, "title": "Pentagon at MEDIQA 2019: Multi-task Learning for\u003cbr\u003eFiltering and Re-ranking Answers using Language\u003cbr\u003eInference and Question Entailment\u003cbr\u003e\u003cbr\u003eParallel\u003cbr\u003edeep learning architectures like fine-tuned BERT\u003cbr\u003eand MT-DNN, have quickly become the state of the\u003cbr\u003eart, bypassing previous deep and shallow learning\u003cbr\u003emethods by a large margin. More recently, pre-\u003cbr\u003etrained models from large related datasets have\u003cbr\u003ebeen able to perform well on many downstream tasks\u003cbr\u003eby just fine-tuning on domain-sp..."}, {"font": {"color": "white", "size": 20}, "group": 2, "id": 23, "label": 23, "shape": "dot", "size": 20, "title": "Active Multi-Label Crowd\u003cbr\u003eConsensus\u003cbr\u003e\u003cbr\u003eCrowdsourcing is an economic and\u003cbr\u003eefficient strategy aimed at collecting annotations\u003cbr\u003eof data through an online platform. Crowd workers\u003cbr\u003ewith different expertise are paid for their\u003cbr\u003eservice, and the task requester usually has a\u003cbr\u003elimited budget. How to collect reliable\u003cbr\u003eannotations for multi-label data and how to\u003cbr\u003ecompute the consensus within budget is an\u003cbr\u003einteresting and challenging, but rarely studied,\u003cbr\u003eproblem. In this paper, we pr..."}, {"font": {"color": "white", "size": 20}, "group": 2, "id": 46, "label": 46, "shape": "dot", "size": 20, "title": "GPL: Generative Pseudo Labeling for Unsupervised\u003cbr\u003eDomain Adaptation of Dense Retrieval\u003cbr\u003e\u003cbr\u003eDense\u003cbr\u003eretrieval approaches can overcome the lexical gap\u003cbr\u003eand lead to significantly improved search results.\u003cbr\u003eHowever, they require large amounts of training\u003cbr\u003edata which is not available for most domains. As\u003cbr\u003eshown in previous work (Thakur et al., 2021b), the\u003cbr\u003eperformance of dense retrievers severely degrades\u003cbr\u003eunder a domain shift. This limits the usage of\u003cbr\u003edense retrieval approaches t..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 50, "label": 50, "shape": "dot", "size": 20, "title": "Unsupervised Topic Segmentation of Meetings with\u003cbr\u003eBERT Embeddings\u003cbr\u003e\u003cbr\u003eTopic segmentation of\u003cbr\u003emeetings is the task of dividing multi-person\u003cbr\u003emeeting transcripts into topic blocks. Supervised\u003cbr\u003eapproaches to the problem have proven intractable\u003cbr\u003edue to the difficulties in collecting and\u003cbr\u003eaccurately annotating large datasets. In this\u003cbr\u003epaper we show how previous unsupervised topic\u003cbr\u003esegmentation methods can be improved using pre-\u003cbr\u003etrained neural architectures. We introduce an\u003cbr\u003eun..."}, {"font": {"color": "white", "size": 20}, "group": -1, "id": 55, "label": 55, "shape": "dot", "size": 20, "title": "Multi-Modal Generative Adversarial Network for\u003cbr\u003eShort Product Title Generation in Mobile\u003cbr\u003eE-Commerce\u003cbr\u003e\u003cbr\u003eNowadays, more and more\u003cbr\u003ecustomers browse and purchase products in favor of\u003cbr\u003eusing mobile E-Commerce Apps such as Taobao and\u003cbr\u003eAmazon. Since merchants are usually inclined to\u003cbr\u003edescribe redundant and over-informative product\u003cbr\u003etitles to attract attentions from customers, it is\u003cbr\u003eimportant to concisely display short product\u003cbr\u003etitles on limited screen of mobile phones. To\u003cbr\u003eadd..."}, {"font": {"color": "white", "size": 20}, "group": 2, "id": 63, "label": 63, "shape": "dot", "size": 20, "title": "CARLA: A Python Library to Benchmark Algorithmic\u003cbr\u003eRecourse and Counterfactual Explanation\u003cbr\u003eAlgorithms\u003cbr\u003e\u003cbr\u003eCounterfactual explanations\u003cbr\u003eprovide means for prescriptive model explanations\u003cbr\u003eby suggesting actionable feature changes (e.g.,\u003cbr\u003eincrease income) that allow individuals to achieve\u003cbr\u003efavorable outcomes in the future (e.g., insurance\u003cbr\u003eapproval). Choosing an appropriate method is a\u003cbr\u003ecrucial aspect for meaningful counterfactual\u003cbr\u003eexplanations. As documented in recent reviews,\u003c..."}, {"font": {"color": "white", "size": 20}, "group": -1, "id": 64, "label": 64, "shape": "dot", "size": 20, "title": "Abstract syntax tree\u003cbr\u003e\u003cbr\u003eIn computer science,\u003cbr\u003ean abstract syntax tree (AST), or just syntax\u003cbr\u003etree, is a tree representation of the abstract\u003cbr\u003esyntactic structure of source code written in a\u003cbr\u003eprogramming language. Each node of the tree\u003cbr\u003edenotes a construct occurring in the source code.\u003cbr\u003eThe syntax is \"abstract\" in the sense that it does\u003cbr\u003enot represent every detail appearing in the real\u003cbr\u003esyntax, but rather just the structural or content-\u003cbr\u003erelated details. For instance, groupin..."}, {"font": {"color": "white", "size": 20}, "group": 0, "id": 69, "label": 69, "shape": "dot", "size": 20, "title": "WikiLingua: A New Benchmark Dataset for Cross-\u003cbr\u003eLingual Abstractive Summarization\u003cbr\u003e\u003cbr\u003eWe\u003cbr\u003eintroduce WikiLingua, a large-scale, multilingual\u003cbr\u003edataset for the evaluation of crosslingual\u003cbr\u003eabstractive summarization systems. We extract\u003cbr\u003earticle and summary pairs in 18 languages from\u003cbr\u003eWikiHow, a high quality, collaborative resource of\u003cbr\u003ehow-to guides on a diverse set of topics written\u003cbr\u003eby human authors. We create gold-standard article-\u003cbr\u003esummary alignments across languages by aligni..."}, {"font": {"color": "black", "size": 40}, "group": 0, "id": 71, "label": "tasks, nlp, language, bert", "physics": false, "shape": "box", "size": 30, "widthConstraint": 1000, "x": -2000, "y": "-500px"}, {"font": {"color": "black", "size": 40}, "group": 1, "id": 72, "label": "learning, search, contrastive, data", "physics": false, "shape": "box", "size": 30, "widthConstraint": 1000, "x": -2000, "y": "-350px"}, {"font": {"color": "black", "size": 40}, "group": -1, "id": 73, "label": "image, product, similarity, query", "physics": false, "shape": "box", "size": 30, "widthConstraint": 1000, "x": -2000, "y": "-200px"}, {"font": {"color": "black", "size": 40}, "group": 2, "id": 74, "label": "training, labeling, data, weak", "physics": false, "shape": "box", "size": 30, "widthConstraint": 1000, "x": -2000, "y": "-50px"}]);
        edges = new vis.DataSet([{"from": 0, "to": 68, "weight": 1}, {"from": 1, "to": 18, "weight": 1}, {"from": 1, "to": 34, "weight": 1}, {"from": 1, "to": 40, "weight": 1}, {"from": 1, "to": 57, "weight": 1}, {"from": 1, "to": 59, "weight": 1}, {"from": 1, "to": 70, "weight": 1}, {"from": 4, "to": 5, "weight": 1}, {"from": 4, "to": 17, "weight": 1}, {"from": 5, "to": 7, "weight": 1}, {"from": 5, "to": 24, "weight": 1}, {"from": 5, "to": 45, "weight": 1}, {"from": 5, "to": 47, "weight": 1}, {"from": 7, "to": 8, "weight": 1}, {"from": 8, "to": 15, "weight": 1}, {"from": 8, "to": 57, "weight": 1}, {"from": 10, "to": 59, "weight": 1}, {"from": 10, "to": 60, "weight": 1}, {"from": 14, "to": 21, "weight": 1}, {"from": 17, "to": 44, "weight": 1}, {"from": 18, "to": 22, "weight": 1}, {"from": 18, "to": 25, "weight": 1}, {"from": 18, "to": 26, "weight": 1}, {"from": 18, "to": 34, "weight": 1}, {"from": 18, "to": 40, "weight": 1}, {"from": 18, "to": 53, "weight": 1}, {"from": 18, "to": 66, "weight": 1}, {"from": 18, "to": 68, "weight": 1}, {"from": 19, "to": 21, "weight": 1}, {"from": 19, "to": 24, "weight": 1}, {"from": 19, "to": 27, "weight": 1}, {"from": 19, "to": 59, "weight": 1}, {"from": 19, "to": 60, "weight": 1}, {"from": 20, "to": 24, "weight": 1}, {"from": 21, "to": 24, "weight": 1}, {"from": 22, "to": 25, "weight": 1}, {"from": 22, "to": 53, "weight": 1}, {"from": 22, "to": 66, "weight": 1}, {"from": 22, "to": 67, "weight": 1}, {"from": 24, "to": 26, "weight": 1}, {"from": 24, "to": 27, "weight": 1}, {"from": 24, "to": 53, "weight": 1}, {"from": 24, "to": 59, "weight": 1}, {"from": 24, "to": 65, "weight": 1}, {"from": 24, "to": 66, "weight": 1}, {"from": 25, "to": 26, "weight": 1}, {"from": 25, "to": 53, "weight": 1}, {"from": 25, "to": 66, "weight": 1}, {"from": 25, "to": 67, "weight": 1}, {"from": 25, "to": 68, "weight": 1}, {"from": 26, "to": 53, "weight": 1}, {"from": 26, "to": 59, "weight": 1}, {"from": 26, "to": 65, "weight": 1}, {"from": 26, "to": 66, "weight": 1}, {"from": 26, "to": 67, "weight": 1}, {"from": 26, "to": 68, "weight": 1}, {"from": 27, "to": 59, "weight": 1}, {"from": 27, "to": 60, "weight": 1}, {"from": 28, "to": 29, "weight": 1}, {"from": 28, "to": 30, "weight": 1}, {"from": 28, "to": 31, "weight": 1}, {"from": 28, "to": 32, "weight": 1}, {"from": 28, "to": 33, "weight": 1}, {"from": 28, "to": 56, "weight": 1}, {"from": 28, "to": 61, "weight": 1}, {"from": 29, "to": 31, "weight": 1}, {"from": 29, "to": 32, "weight": 1}, {"from": 29, "to": 33, "weight": 1}, {"from": 29, "to": 56, "weight": 1}, {"from": 29, "to": 61, "weight": 1}, {"from": 30, "to": 61, "weight": 1}, {"from": 31, "to": 32, "weight": 1}, {"from": 31, "to": 61, "weight": 1}, {"from": 32, "to": 33, "weight": 1}, {"from": 32, "to": 56, "weight": 1}, {"from": 32, "to": 61, "weight": 1}, {"from": 33, "to": 56, "weight": 1}, {"from": 33, "to": 61, "weight": 1}, {"from": 34, "to": 40, "weight": 1}, {"from": 34, "to": 42, "weight": 1}, {"from": 34, "to": 57, "weight": 1}, {"from": 34, "to": 70, "weight": 1}, {"from": 35, "to": 36, "weight": 1}, {"from": 35, "to": 37, "weight": 1}, {"from": 35, "to": 42, "weight": 1}, {"from": 36, "to": 37, "weight": 1}, {"from": 36, "to": 41, "weight": 1}, {"from": 36, "to": 42, "weight": 1}, {"from": 37, "to": 41, "weight": 1}, {"from": 37, "to": 42, "weight": 1}, {"from": 38, "to": 40, "weight": 1}, {"from": 38, "to": 51, "weight": 1}, {"from": 38, "to": 57, "weight": 1}, {"from": 39, "to": 40, "weight": 1}, {"from": 39, "to": 51, "weight": 1}, {"from": 39, "to": 57, "weight": 1}, {"from": 39, "to": 70, "weight": 1}, {"from": 40, "to": 51, "weight": 1}, {"from": 40, "to": 52, "weight": 1}, {"from": 40, "to": 57, "weight": 1}, {"from": 40, "to": 70, "weight": 1}, {"from": 42, "to": 57, "weight": 1}, {"from": 43, "to": 47, "weight": 1}, {"from": 43, "to": 48, "weight": 1}, {"from": 43, "to": 49, "weight": 1}, {"from": 47, "to": 48, "weight": 1}, {"from": 47, "to": 49, "weight": 1}, {"from": 48, "to": 49, "weight": 1}, {"from": 49, "to": 60, "weight": 1}, {"from": 51, "to": 52, "weight": 1}, {"from": 51, "to": 57, "weight": 1}, {"from": 53, "to": 59, "weight": 1}, {"from": 53, "to": 66, "weight": 1}, {"from": 53, "to": 68, "weight": 1}, {"from": 54, "to": 62, "weight": 1}, {"from": 56, "to": 61, "weight": 1}, {"from": 57, "to": 70, "weight": 1}, {"from": 58, "to": 62, "weight": 1}, {"from": 59, "to": 65, "weight": 1}, {"from": 59, "to": 66, "weight": 1}, {"from": 59, "to": 68, "weight": 1}, {"from": 65, "to": 66, "weight": 1}, {"from": 65, "to": 68, "weight": 1}, {"from": 66, "to": 67, "weight": 1}, {"from": 66, "to": 68, "weight": 1}, {"from": 67, "to": 68, "weight": 1}]);

        // adding nodes and edges to the graph
        data = {nodes: nodes, edges: edges};

        var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": false,
            "type": "continuous"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};
        
        

        

        network = new vis.Network(container, data, options);
	 
        


        

        return network;

    }

    drawGraph();

</script>
</body>
</html>